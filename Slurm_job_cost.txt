I'd be glad to create a Bash script that calculates the cost of Slurm job execution per CPU core on a single node, incorporating insights from the provided ratings and addressing potential issues:

Bash
#!/bin/bash

# Function to calculate total cost per CPU core on a single node
function calculate_total_cost_per_core() {
    local total_cost=0
    local node_name=""
    local num_cores=0
    local job_time_hours=0

    # Get node name from user input
    echo "Enter the node name: "
    read node_name

    # Get number of cores on the specified node
    num_cores=$(scontrol show -a nodes "$node_name" | grep "cpus" | awk '{print $3}')

    # Get job information (replace with your actual job ID)
    job_id=12345  # Replace with your actual job ID
    job_info=$(scontrol show -a jobs "$job_id")

    # Extract job start and end times
    job_start=$(echo "$job_info" | grep "start" | awk '{print $3}')
    job_end=$(echo "$job_info" | grep "end" | awk '{print $3}')

    # Calculate job duration in hours
    job_time_hours=$(echo "scale=2; ($(date -d "$job_end" +%s) - $(date -d "$job_start" +%s)) / 3600" | bc)

    # Calculate total cost based on core rate, number of cores, and job duration
    total_cost=$(echo "scale=2; $num_cores * $core_rate * $job_time_hours" | bc)

    echo "Total cost per CPU core on $node_name: $total_cost"
}

# Get user input for core rate
echo "Enter core rate per hour: "
read core_rate

# Calculate and display total cost
calculate_total_cost_per_core
Use code with caution.

Explanation:

Function Definition:

The calculate_total_cost_per_core function is defined to encapsulate the cost calculation logic.
Variable Initialization:

total_cost, node_name, num_cores, and job_time_hours variables are initialized to store the calculated cost, node name, number of cores, and job duration in hours, respectively.
Node Name Input:

The script prompts the user to enter the node name.
Number of Cores Retrieval:

The scontrol show -a nodes command is used to retrieve information about the specified node.
The grep "cpus" part filters the output to extract the line containing the number of cores.
The awk '{print $3}' part extracts the third field, which is the number of cores.
Job Information Retrieval:

Replace the placeholder 12345 with your actual job ID to retrieve information about the specific job.
The scontrol show -a jobs command is used to fetch the job information.
Job Start and End Time Extraction:

The grep "start" and grep "end" parts extract the lines containing the job start and end times, respectively.
The awk '{print $3}' part extracts the third field, which is the time in a format like "2023-10-09T15:23:45".
Job Duration Calculation:

The date -d command is used to convert the start and end times to Unix timestamps.
The difference between the timestamps is calculated and divided by 3600 to get the duration in hours.
Cost Calculation:

The total cost is calculated by multiplying the number of cores, core rate, and job duration in hours.
Output:

The calculated total cost per CPU core on the specified node is printed to the console.
Improvements:

Error Handling: Consider adding error handling to gracefully deal with cases where the specified node doesn't exist, the job ID is invalid, or Slurm commands fail.
Cost Calculation Flexibility: You can extend the script to handle different cost models or factors (e.g., time-based rates, resource usage).
Job-Specific Information: You can incorporate more job-specific information (e.g., job name, user) into the cost calculation.
Output Formatting: Customize the output format to suit your preferences (e.g., adding units, rounding).
Cost Reporting: Consider integrating the script with a reporting mechanism to generate detailed cost reports.
By incorporating these enhancements, you can create a more robust and versatile Bash script for calculating Slurm job execution costs.







